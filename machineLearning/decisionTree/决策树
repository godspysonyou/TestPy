决策树
	优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
	缺点：可能会产生过度匹配问题
	使用数据类型：数值型和标称型（树构造算法只适用于标称型数据，因此数值型数据必须离散化）
	优势：相比于kNN算法，（kNN算法无法给出数据的内在含义），决策树的主要优势是在于数据形式给常容易理解
	
	问题：
		概述：在构造决策树时，我们需要解决的第一个问题就是，的那个钱数据集上那个特征在划分数据分类时起决定性作用。为了找到决定性的特征，我们必须评估每个特征。
		解决：信息论，信息增益，ID3划分
			每次划分数据集我们只选取一个特征属性，如果训练集中存在20个特征，第一次我们选择哪个特征作为划分的参考属性呢？以后选哪个呢？
		
	创建分支的伪代码函数createBranch()如下：
		检测数据集中的每个子项是否属于同一分类：
			if so return 类标签;
			else  
				寻找划分数据集的最好特征
				划分数据集
				创建分支节点
					for 每个划分的子集
						调用函数createBranch()并增加返回结果到分支节点中
				return 分支节点
				
	如何划分数据集？
		划分数据集的大原则是：
			将无需的数据变得更加有序。我们使用信息论量化度量信息的内容。
		信息论：
			通常一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。概率大，出现机会多，不确定性就小。反之，不确定性就大。
			不确定性函数f是概率p的单调递降函数；两个独立符号所产生的不确定性应等于，各自不确定性之和，即f(p1,p2)=f(p1)+f(p2),这称为可加性。
			同时满足这两个条件的函数f是对数函数，即f(p)=log(1/p)=-log(p)
			在信源中，考虑的不是某一单个符号的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。假设信源x有n种取值情况：u1,u2,u3,,,un,对应的概率为：
			p1,p2,,,pn,且各种符号的出现彼此独立。这是，信源的平均不确定性应该为单个符号不确定性-log(pi)的统计平均值，可称为信息熵，即H(u)=E[-log(pi)]=-sigma pi*log(pi)
		信息增益：
			在划分数据集前后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集之后获得的信息增益，获得信息增益更高的特征就是最好的选择
		熵（entropy）：
			克劳德-香农	，熵定义为信息的期望值。如果待分类的事务可能划分在多个分类之中，则符号xi的信息定义为：
				l(xi)=-log2的p(xi) 			p(xi)是该分类的概率
			为了计算熵，我们需要计算所有类别所有可能值包括的信息期望值，通过以下公式得到：
				H=-sigma[i=1~n]p(xi)log2的p(xi)  其中n是分类的数目
			拓展：另一个度量集合无序程度的方法是基尼不纯度（Gini impurity），简单说就是从一个数据集中随即选取子项，度量其被错误分类到其他组的概率
			
		划分数据集：
			我们对每个特征值划分数据集的结果计算一次信息熵，然后判断按照那个特征划分数据集是最好的方式
			
		决策树的主要优点是直观易于理解	
		
		决策树可以将分类器存储在硬盘上，而不用每次对数据分类时重新学习一遍，这也是决策树的有点之一，像k-NN算法就无法持久化分类。我们
		可以预先提炼并存储数据集中包含的知识信息
			